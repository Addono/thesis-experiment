{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from typing import Optional, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', None)\n",
    "\n",
    "class Metric:\n",
    "    name: str\n",
    "    abbreviation: str\n",
    "    unit: Optional[str]\n",
    "\n",
    "    def __init__(self, name: str, abbreviation: str, unit: Optional[str]) -> None:\n",
    "        self.name = name\n",
    "        self.abbreviation = abbreviation\n",
    "        self.unit = unit\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        base = f\"{self.name} ({self.abbreviation})\"\n",
    "\n",
    "        if self.unit:\n",
    "            return f\"{base} [{self.unit}]\"\n",
    "        else:\n",
    "            return base\n",
    "\n",
    "OUT_DIR = \"output\"\n",
    "\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)\n",
    "\n",
    "APPLICATIONS = {\n",
    "    \"postgresql-ha\": \"PostgreSQL\",\n",
    "    \"redis\": \"Redis\",\n",
    "    # \"redis-cluster\": \"Redis Cluster,\n",
    "}\n",
    "EXPERIMENTS = {\n",
    "    \"ct\": \"Control Test\",\n",
    "    \"pd\": \"Perturbation Disabled\",\n",
    "    \"vb\": \"Vary Build\",\n",
    "    \"vp\": \"Vary Patch\",\n",
    "    \"vm\": \"Vary Minor\",\n",
    "}\n",
    "EXPERIMENTS_ORDER = { value: index for index, value in enumerate(EXPERIMENTS.keys()) }\n",
    "EXPERIMENTS_NAME_ORDER = { EXPERIMENTS[experiment]: EXPERIMENTS_ORDER[experiment] for experiment in EXPERIMENTS_ORDER.keys()}\n",
    "METRICS: Dict[str, Metric] = {\n",
    "    \"time_to_initialize\": Metric(\n",
    "        name=\"Time to Complete Initialization\",\n",
    "        abbreviation=\"TCI\",\n",
    "        unit=\"s\",\n",
    "        ),\n",
    "    \"time_to_first_request\": Metric(\n",
    "        name=\"Time to First Request\",\n",
    "        abbreviation=\"TFR\",\n",
    "        unit=\"s\",\n",
    "    ),\n",
    "    \"time_to_all_requests\": Metric(\n",
    "        name=\"Time to All Containers Handle Requests\",\n",
    "        abbreviation=\"TACHR\",\n",
    "        unit=\"s\",\n",
    "    ),\n",
    "    \"restarts\": Metric(\n",
    "        name=\"Amount of Container Restarts\",\n",
    "        abbreviation=\"ACR\",\n",
    "        unit=None,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Import all data\n",
    "\n",
    "try:\n",
    "    # Use the current file as the base-path\n",
    "    base_path = os.path.dirname(__file__)\n",
    "except NameError:\n",
    "    # Fall back to the current directory if it is not available,\n",
    "    # is it means that we are in interactive mode.\n",
    "    base_path = \".\"\n",
    "\n",
    "\n",
    "# Read all datafiles as separate DataFrames\n",
    "dfs = {\n",
    "    (application_name, experiment): pd.read_json(path, lines=True)\n",
    "    if os.path.isfile(path := os.path.join(base_path, \"..\", \"results\", f\"{experiment}_{application}.jsonl\")) else None\n",
    "    for experiment in EXPERIMENTS.keys()\n",
    "    for application, application_name in APPLICATIONS.items()\n",
    "}\n",
    "\n",
    "# Combine all DataFrames\n",
    "df = pd.concat(dfs, names=[\"Application\", \"experiment\"])\n",
    "\n",
    "# Adds the experiment and application index as a column\n",
    "df.reset_index(level=[0, 1], inplace=True)\n",
    "\n",
    "# Adds a human readable experiment name column\n",
    "df[\"Experiment Type\"] = df[\"experiment\"].apply(lambda x: EXPERIMENTS[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define methods to make and store plots\n",
    "\n",
    "def save_plot(name: str):\n",
    "    for file_type in [\"pdf\", \"png\", \"jpg\", \"svg\"]:\n",
    "        plt.savefig(os.path.join(OUT_DIR, f\"{name}.{file_type}\"),\n",
    "                    transparent=True,\n",
    "                    bbox_inches='tight'\n",
    "                    )\n",
    "\n",
    "def show_plot(data: DataFrame, x: str, y: str, xlabel: str, ylabel: str, hue: Optional[str] = None) -> None:\n",
    "    sns.set(style=\"ticks\")\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(12, 3))\n",
    "\n",
    "    sns.boxplot(data=data,\n",
    "                x=x,\n",
    "                y=y,\n",
    "                hue=hue,\n",
    "                )\n",
    "\n",
    "    sns.swarmplot(data=data,\n",
    "                  x=x,\n",
    "                  y=y,\n",
    "                  size=3,\n",
    "                  hue=hue,\n",
    "                  alpha=0.5,\n",
    "                  )\n",
    "\n",
    "    ax.xaxis.grid(True)\n",
    "    ax.set_xlim(xmin=-1,)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# All Raw Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display the data loaded\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Amount of Experiment Iterations\n",
    "\n",
    "Shows the amount of times each experiment has been ran.\n",
    "\n",
    "*Note: Redis does not have any results for \"Vary Minor\", as no suitable minor version is available to test with.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a pivot table to count the amount of experiment iterations\n",
    "pivot = df.pivot_table(index=\"Experiment Type\",\n",
    "                       columns=\"Application\",\n",
    "                       values=\"restarts\",\n",
    "                       aggfunc=np.size,\n",
    "                       fill_value=\"N/A\",\n",
    "                       margins=True,\n",
    "                       margins_name=\"Total\")\n",
    "\n",
    "# Sort the rows of the pivot table based on the perturbation intensity of the experiments\n",
    "sorted_index = sorted(pivot.index, key=lambda v: EXPERIMENTS_NAME_ORDER.get(v, math.inf))\n",
    "pivot.reindex(sorted_index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Average Time to Initialize (TTI) and Amount of Restarts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.groupby([\"Experiment Type\", \"Application\"], sort=False)\\\n",
    "    [[\"time_to_initialize\", \"restarts\"]]\\\n",
    "    .mean()\\\n",
    "    .round(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Control Test vs. Perturbation Disabled"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for metric in METRICS.keys():\n",
    "    show_plot(data=df[(df.experiment.isin([\"ct\", \"pd\"]))],\n",
    "              x=metric,\n",
    "              y=\"Application\",\n",
    "              xlabel=str(METRICS[metric]),\n",
    "              ylabel=\"Applications\",\n",
    "              hue=\"Experiment Type\"\n",
    "              )\n",
    "    save_plot(f\"ct_vs_pd-{metric}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Statistical Significance\n",
    "\n",
    "### By Metric & Application\n",
    "\n",
    "Calculate the p-value for the hypothesis that the mean of the metric are the same between the Control Test and Perturbation Disabled. Welch's t-test is used, as not to assume that the variance is equal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pvalues = pd.DataFrame(({\n",
    "    \"p-Value\": scipy.stats.ttest_ind(*(df[(df.experiment == experiment) & (df.Application == application)][metric]\n",
    "                                        for experiment in (\"ct\", \"pd\",)\n",
    "                                       ),\n",
    "                                      equal_var=True\n",
    "                                      ).pvalue,\n",
    "    \"Metric\": str(METRICS[metric]),\n",
    "    \"Application\": application,\n",
    "    }\n",
    "for metric in filter(lambda x: x != \"restarts\", METRICS.keys())\n",
    "for application in (\"PostgreSQL\", \"Redis\")\n",
    "))\n",
    "\n",
    "pvalues.pivot(\"Metric\", \"Application\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combined\n",
    "These p-values can be combined using the Fisher's method, which yields a combined probabilty of:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(_, combined_pvalue) = scipy.stats.combine_pvalues(pvalues[\"p-Value\"])\n",
    "\n",
    "combined_pvalue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Impact of Perturbation on the Collected Metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for metric in METRICS.keys():\n",
    "    show_plot(data=df,\n",
    "              x=metric,\n",
    "              y=\"Experiment Type\",\n",
    "              xlabel=str(METRICS[metric]),\n",
    "              ylabel=\"Experiment Type\",\n",
    "              hue=\"Application\"\n",
    "              )\n",
    "    save_plot(f\"box_and_scatter_plot-{metric}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}